{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56c256b4-7de9-4617-937c-92878bebfc26",
    "_uuid": "52ec7acbe234e42574ce9b28ccc4047e9e0ac268"
   },
   "source": [
    "My goals in this Kernel are as follows:\n",
    "\n",
    "* I will use the awesome kernels published by great Kagglers for this competition. I will try to explain why each step is neccessary, especially for data preparation. I have used the amazing work of following users:  \n",
    "-[the1owe](https://www.kaggle.com/the1owl/surprise-me)  \n",
    "-[Bojan Tunguz](https://www.kaggle.com/tunguz/surprise-me-2/code)  \n",
    "* I will test several algorithms and I will  take advantage of `RandomizedSearchCV` class of `Scikit-learn` for hyper parameter optimization. Randomized search perfomrs drastically better than grid search. Please see [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "61fceb69-a3c2-48e3-9d03-766332dd59ad",
    "_uuid": "c6f23a7b6a6012e3976954bd961571865bf05092",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob,re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from xgboost import XGBRegressor\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import h2o\n",
    "#from h2o.automl import H2OAutoML\n",
    "\n",
    "import warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    \n",
    "#address = '../input/'\n",
    "address= 'C:\\\\Users\\\\noori\\\\Dropbox (MIT)\\\\DS\\\\Kaggle\\\\RecruitRestaurent' #windows\n",
    "#address='/Users/MehD/Dropbox (MIT)/DS/Kaggle/RecruitRestaurent' #mac\n",
    "os.chdir(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "9a10c987-66c1-4f01-af18-179147d64845",
    "_uuid": "76f3d9ea7d130182137601d5231d5e3273044005",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assiging a name to each data frame\n",
    "data={\n",
    "    'tra':pd.read_csv('air_visit_data.csv'),\n",
    "    'as':pd.read_csv('air_store_info.csv'),\n",
    "    'hs':pd.read_csv('hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('air_reserve.csv'),\n",
    "    'hr': pd.read_csv('hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('sample_submission.csv'),\n",
    "    'hol':pd.read_csv('date_info.csv').rename(columns={'calendar_date':'visitor_date'})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "06056b6a-c209-4d00-83ce-89993108c587",
    "_uuid": "3cf67f6e0622e68cb40ea09edbc5cf41eecf18ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's add to the hpg_reserve the ids from id dataset\n",
    "data['hr']=data['hr'].merge(data['id'],on=['hpg_store_id'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "9d6f9aa1-a0dd-4ce1-9ebd-8d0e475ee443",
    "_uuid": "276a52b033c9731512e0cca9253aa0c09be5e84f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's tranfrom date to datetime objects. Please note, to_datetime also includes the actual time. Using .dt.date we only capture date.\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime']).dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime']).dt.date\n",
    "    \n",
    "    #here, we are actually engineering a new feature that captures the difference between visit and reserve times\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    \n",
    "    #let's group datasets by id and visit date, then get the sum and mean of reserve and reserve differnce, then rename the columns\n",
    "    temp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    \n",
    "    temp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    #now let's merge these two new temp dataframes.\n",
    "    data[df]=temp1.merge(temp2,how='inner',on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "90f8a41a-28f8-4137-aa79-b4c6c8e51bb1",
    "_uuid": "948211eaf45911508d91fb00ab0da039609f2e65",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['tra']['visit_date']=pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow']=data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year']=data['tra']['visit_date'].dt.year\n",
    "data['tra']['month']=data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date']=data['tra']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be977952-1720-45a3-a051-0a164d70148d",
    "_uuid": "b5b374fdc925453932e8f0c7a7e0e14c47f94c70"
   },
   "source": [
    "We do the same thing for test set. Please note, we should first split the test ids and get dates and ids seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "aca4e819-b199-4c4d-bf3f-d6930d3f511a",
    "_uuid": "ac2b5e144fc8e0a4bf2ae33b33406d238a4cbd06",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "25a8ed7b-854a-4a98-a305-03c112c6c801",
    "_uuid": "4164a87bf86e064cb17c39d471158aed50f60a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique stores is: 821\n",
      "total number of data records in test set is 32019\n"
     ]
    }
   ],
   "source": [
    "unique_stores=data['tes']['air_store_id'].unique()\n",
    "print('The number of unique stores is:', unique_stores.shape[0])\n",
    "print('total number of data records in test set is',data['tes'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5e9d455-6ed1-4701-9eec-09e0946c22d5",
    "_uuid": "d4644de1fcff2404cf20af35ca7df2af86c86958"
   },
   "source": [
    "Now, we'd like to create a new dataframe, that has 7\\*821 rows, for each 7 days of the week. Later, we will add values to this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "8d5aefbd-b990-4282-9c02-084a733e2749",
    "_uuid": "baeae469922e4d1a497e272bd29ca0c1d83b5135",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores=pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)],\n",
    "            axis=0,ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We will add a new feature to the train set. This feature is taken from the competition conversations. Please see [here](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/46179).\n",
    "This feature is an exponentially weighted rolling average of the number of visitors.\n",
    "\n",
    "We should find a way to replace the missing values. For this purpose, I first calculate the mean of ewm, then create a new id for both mean_ewm and data['tra'] datasets, and then set the index to the new id, and then replace the null values with the mean of ewm.\n",
    "\n",
    "**Note** please let me know if you think of any better way to perform this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['test_ewm']=pd.read_csv('test_ewm.csv')\n",
    "mean_ewm={}\n",
    "\n",
    "def calc_shifted_ewm(series, alpha, adjust=True):\n",
    "    return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n",
    "\n",
    "for df in ['tra','test_ewm']:\n",
    "    data[df]['ewm'] = data[df].groupby(['air_store_id', 'dow'])\\\n",
    "                  .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1)).sort_index(level=['air_store_id','dow']).values\n",
    "    \n",
    "    #finding the mean of ewm\n",
    "    mean_ewm[df]=data[df].groupby(['air_store_id','dow']).mean().reset_index()\n",
    "    \n",
    "    #setting new index for new_ewm\n",
    "    mean_ewm[df]['id_dow']=mean_ewm[df].apply(lambda x: '_'.join([str(x['air_store_id']),str(x['dow'])]),axis=1)\n",
    "    mean_ewm[df]=mean_ewm[df].set_index('id_dow')\n",
    "    \n",
    "    #setting new index for data['tra']\n",
    "    data[df]['id_dow']=data[df].apply(lambda x: '_'.join([str(x['air_store_id']),str(x['dow'])]),axis=1)\n",
    "    data[df]=data[df].set_index('id_dow')\n",
    "    \n",
    "    #filling na\n",
    "    data[df]['ewm']=data[df]['ewm'].fillna(mean_ewm[df]['ewm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merging new ewm with test set.\n",
    "data['tes']=data['tes'].merge(data['test_ewm'],on=['id'],how='left')\n",
    "data['tes']=data['tes'][['id','visitors_x','visit_date','air_store_id_x','dow_x','year','month','ewm']]\n",
    "data['tes']=data['tes'].rename(columns={'visitors_x':'visitors','air_store_id_x':'air_store_id','dow_x':'dow'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here' we calculate min, max, median, mean, and the number of times each store has been visited per each day of the week. The following code might sound complicated but we are using the aggregate method of groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp=data['tra'].groupby(['air_store_id','dow']).agg({'visitors':[np.min, np.mean, np.median, np.max, np.size]}).reset_index()\n",
    "\n",
    "temp.columns = ['air_store_id', 'dow', 'min_visitors', 'mean_visitors', 'median_visitors','max_visitors','count_observations']\n",
    "\n",
    "stores=stores.merge(temp, on=['air_store_id','dow'],how='left')\n",
    "# let's add the store information to this dataframe. Including, genre, name, latitude, and longtitude.\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "301d7934-fbc6-4930-859f-8ba4ff618e7c",
    "_uuid": "5106fecd533ced9cda3d5a3083f631a966d6feaf"
   },
   "source": [
    "Let's create new features based on name and area. We use [`LabelEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) class of preprocessing in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_cell_guid": "79837ea4-4e9a-4294-8e15-65c19698a532",
    "_uuid": "51801bcf5e09a43ab09ca1fbd4043cec4d2a8449",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores['prefecture'] = stores['air_area_name'].map(lambda x: str(x).split(' ')[0]) \n",
    "\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "stores['prefecture']=lbl.fit_transform(stores['prefecture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>dow</th>\n",
       "      <th>min_visitors</th>\n",
       "      <th>mean_visitors</th>\n",
       "      <th>median_visitors</th>\n",
       "      <th>max_visitors</th>\n",
       "      <th>count_observations</th>\n",
       "      <th>air_genre_name</th>\n",
       "      <th>air_area_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>prefecture</th>\n",
       "      <th>air_genre_name0</th>\n",
       "      <th>air_area_name0</th>\n",
       "      <th>air_genre_name1</th>\n",
       "      <th>air_area_name1</th>\n",
       "      <th>air_genre_name2</th>\n",
       "      <th>air_area_name2</th>\n",
       "      <th>air_genre_name3</th>\n",
       "      <th>air_area_name3</th>\n",
       "      <th>air_genre_name4</th>\n",
       "      <th>air_area_name4</th>\n",
       "      <th>air_genre_name5</th>\n",
       "      <th>air_area_name5</th>\n",
       "      <th>air_genre_name6</th>\n",
       "      <th>air_area_name6</th>\n",
       "      <th>air_genre_name7</th>\n",
       "      <th>air_area_name7</th>\n",
       "      <th>air_genre_name8</th>\n",
       "      <th>air_area_name8</th>\n",
       "      <th>air_genre_name9</th>\n",
       "      <th>air_area_name9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.457143</td>\n",
       "      <td>19.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_0164b9927d20bcc3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_0241aa3964b7f861</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.920635</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>11</td>\n",
       "      <td>84</td>\n",
       "      <td>35.712607</td>\n",
       "      <td>139.779996</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_0328696196e46f18</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.416667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8</td>\n",
       "      <td>101</td>\n",
       "      <td>34.701279</td>\n",
       "      <td>135.528090</td>\n",
       "      <td>101</td>\n",
       "      <td>8</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_034a3d5b40d5b1b1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.864865</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>34.692337</td>\n",
       "      <td>135.472229</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  dow  min_visitors  mean_visitors  median_visitors  \\\n",
       "0  air_00a91d42b08b08d9    0           1.0      22.457143             19.0   \n",
       "1  air_0164b9927d20bcc3    0           2.0       7.500000              6.0   \n",
       "2  air_0241aa3964b7f861    0           2.0       8.920635              8.0   \n",
       "3  air_0328696196e46f18    0           2.0       6.416667              4.0   \n",
       "4  air_034a3d5b40d5b1b1    0           1.0      11.864865             10.0   \n",
       "\n",
       "   max_visitors  count_observations  air_genre_name  air_area_name   latitude  \\\n",
       "0          47.0                35.0              10             42  35.694003   \n",
       "1          19.0                20.0              10             62  35.658068   \n",
       "2          23.0                63.0              11             84  35.712607   \n",
       "3          27.0                12.0               8            101  34.701279   \n",
       "4          66.0                37.0               6              5  34.692337   \n",
       "\n",
       "    longitude  prefecture  air_genre_name0  air_area_name0  air_genre_name1  \\\n",
       "0  139.753595          42               10              42                0   \n",
       "1  139.751599          62               10              62                0   \n",
       "2  139.779996          84               11              84                0   \n",
       "3  135.528090         101                8             101                0   \n",
       "4  135.472229           5                6               5                0   \n",
       "\n",
       "   air_area_name1  air_genre_name2  air_area_name2  air_genre_name3  \\\n",
       "0               0                0               0                0   \n",
       "1               0                0               0                0   \n",
       "2               0                0               0                0   \n",
       "3               0                0               0                0   \n",
       "4               0                0               0                0   \n",
       "\n",
       "   air_area_name3  air_genre_name4  air_area_name4  air_genre_name5  \\\n",
       "0               0                0               0                0   \n",
       "1               0                0               0                0   \n",
       "2               0                0               0                0   \n",
       "3               0                0               0                0   \n",
       "4               0                0               0                0   \n",
       "\n",
       "   air_area_name5  air_genre_name6  air_area_name6  air_genre_name7  \\\n",
       "0               0                0               0                0   \n",
       "1               0                0               0                0   \n",
       "2               0                0               0                0   \n",
       "3               0                0               0                0   \n",
       "4               0                0               0                0   \n",
       "\n",
       "   air_area_name7  air_genre_name8  air_area_name8  air_genre_name9  \\\n",
       "0               0                0               0                0   \n",
       "1               0                0               0                0   \n",
       "2               0                0               0                0   \n",
       "3               0                0               0                0   \n",
       "4               0                0               0                0   \n",
       "\n",
       "   air_area_name9  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d573e4ed-1e53-4bad-870c-b8dcedb2b6f8",
    "_uuid": "e2c2951e811c30c5e77f02eebba8efff3e034bd7"
   },
   "source": [
    "Successfully label encoded. Let's also label holidays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "e4e08738-7e1a-4c85-acb6-a7bb7573255c",
    "_uuid": "e1f270b49584adae36698e0198eb7a38e227ae92",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hol']['visit_date']=pd.to_datetime(data['hol']['visitor_date'])\n",
    "data['hol']['day_of_week']=lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date']=data['hol']['visit_date'].dt.date\n",
    "data['hol']=data['hol'].drop('visitor_date',axis=1)\n",
    "\n",
    "#merge the holiday flags to train and test sets.\n",
    "train=data['tra'].merge(data['hol'],on=['visit_date'],how='left')\n",
    "test=data['tes'].merge(data['hol'],on=['visit_date'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_cell_guid": "496967d5-4bf9-4909-9f2d-c12f669db1e5",
    "_uuid": "4202f02534d50fa4575ef04eafffe091a2631d5c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=train.merge(stores,how='left',on=['air_store_id','dow'])\n",
    "test=test.merge(stores,how='left',on=['air_store_id','dow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_cell_guid": "5533a9ab-550f-4cb6-841b-ab7b3b1ff95e",
    "_uuid": "19eba6f70965153d6291cb19b40e63b72896f7d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "601be7f5-20e6-48c1-87f9-4da0c4edc929",
    "_uuid": "ae27c4e530ba8c64d488c53f211941fcf6e0a5e0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_cell_guid": "5f426cfa-b0ac-4496-a847-66c4878b05bb",
    "_uuid": "a20e70ccf60d5ce020918a6f8ba31f8dd18793b2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#engineering new features\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "9e1f906e-181c-44e0-b47e-6c26b03760fe",
    "_uuid": "1aa53f7e4227557bd01608f9ae290f6802a6041e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# engineeirng new features, Please refer to original codes mentioned in the introduction for more info.\n",
    "\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_cell_guid": "2f22bcd6-ea27-4453-9b4e-156d2d92abda",
    "_uuid": "c328430a7e2572700042e4b7ad6a5736c8cfa47f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_cell_guid": "d694d789-cae7-4f77-ac33-10950ee07109",
    "_uuid": "9d0e91ff8c776804461fd7d8e946c24c3bce007d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_cell_guid": "9b8cec31-b1fe-4cf0-abde-a850dcb5a33c",
    "_uuid": "5c6df8fd4bdf6e295e58b79cb1026b79ed8fc56d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features are:  51\n"
     ]
    }
   ],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors','air_genre_name','']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "#let's see how many features are we traning on\n",
    "print('number of features are: ', len(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_cell_guid": "fae898c9-f095-4ae7-a619-d987d5b624da",
    "_uuid": "b68fd108343128e7b49004d0a896ea5b36b4ebf4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGB starter template borrowed from @anokas: https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655\n",
    "\n",
    "for c, dtype in zip(train.columns, train.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>ewm</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>holiday_flg</th>\n",
       "      <th>min_visitors</th>\n",
       "      <th>mean_visitors</th>\n",
       "      <th>median_visitors</th>\n",
       "      <th>max_visitors</th>\n",
       "      <th>count_observations</th>\n",
       "      <th>air_area_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>prefecture</th>\n",
       "      <th>air_genre_name0</th>\n",
       "      <th>air_area_name0</th>\n",
       "      <th>air_genre_name1</th>\n",
       "      <th>air_area_name1</th>\n",
       "      <th>air_genre_name2</th>\n",
       "      <th>air_area_name2</th>\n",
       "      <th>air_genre_name3</th>\n",
       "      <th>air_area_name3</th>\n",
       "      <th>air_genre_name4</th>\n",
       "      <th>air_area_name4</th>\n",
       "      <th>air_genre_name5</th>\n",
       "      <th>air_area_name5</th>\n",
       "      <th>air_genre_name6</th>\n",
       "      <th>air_area_name6</th>\n",
       "      <th>air_genre_name7</th>\n",
       "      <th>air_area_name7</th>\n",
       "      <th>air_genre_name8</th>\n",
       "      <th>air_area_name8</th>\n",
       "      <th>air_genre_name9</th>\n",
       "      <th>air_area_name9</th>\n",
       "      <th>rs1_x</th>\n",
       "      <th>rv1_x</th>\n",
       "      <th>rs2_x</th>\n",
       "      <th>rv2_x</th>\n",
       "      <th>rs1_y</th>\n",
       "      <th>rv1_y</th>\n",
       "      <th>rs2_y</th>\n",
       "      <th>rv2_y</th>\n",
       "      <th>total_reserv_sum</th>\n",
       "      <th>total_reserv_mean</th>\n",
       "      <th>total_reserv_dt_diff_mean</th>\n",
       "      <th>date_int</th>\n",
       "      <th>var_max_lat</th>\n",
       "      <th>var_max_long</th>\n",
       "      <th>lon_plus_lat</th>\n",
       "      <th>air_store_id2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>19.026867</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.843750</td>\n",
       "      <td>25.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>35.65807</td>\n",
       "      <td>139.751602</td>\n",
       "      <td>62.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20160113</td>\n",
       "      <td>8.362564</td>\n",
       "      <td>4.521799</td>\n",
       "      <td>175.409668</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.292307</td>\n",
       "      <td>21.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>35.65807</td>\n",
       "      <td>139.751602</td>\n",
       "      <td>62.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20160114</td>\n",
       "      <td>8.362564</td>\n",
       "      <td>4.521799</td>\n",
       "      <td>175.409668</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>22.631578</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.738461</td>\n",
       "      <td>35.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>35.65807</td>\n",
       "      <td>139.751602</td>\n",
       "      <td>62.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20160115</td>\n",
       "      <td>8.362564</td>\n",
       "      <td>4.521799</td>\n",
       "      <td>175.409668</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>20.184502</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.651516</td>\n",
       "      <td>27.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>35.65807</td>\n",
       "      <td>139.751602</td>\n",
       "      <td>62.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20160116</td>\n",
       "      <td>8.362564</td>\n",
       "      <td>4.521799</td>\n",
       "      <td>175.409668</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>18.967724</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.754386</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>35.65807</td>\n",
       "      <td>139.751602</td>\n",
       "      <td>62.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20160118</td>\n",
       "      <td>8.362564</td>\n",
       "      <td>4.521799</td>\n",
       "      <td>175.409668</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dow  year  month        ewm  day_of_week  holiday_flg  min_visitors  \\\n",
       "0    2  2016      1  19.026867            6            0           7.0   \n",
       "1    3  2016      1  20.000000            4            0           2.0   \n",
       "2    4  2016      1  22.631578            0            0           4.0   \n",
       "3    5  2016      1  20.184502            2            0           6.0   \n",
       "4    0  2016      1  18.967724            1            0           2.0   \n",
       "\n",
       "   mean_visitors  median_visitors  max_visitors  count_observations  \\\n",
       "0      23.843750             25.0          57.0                64.0   \n",
       "1      20.292307             21.0          54.0                65.0   \n",
       "2      34.738461             35.0          61.0                65.0   \n",
       "3      27.651516             27.0          53.0                66.0   \n",
       "4      13.754386             12.0          34.0                57.0   \n",
       "\n",
       "   air_area_name  latitude   longitude  prefecture  air_genre_name0  \\\n",
       "0           62.0  35.65807  139.751602        62.0              8.0   \n",
       "1           62.0  35.65807  139.751602        62.0              8.0   \n",
       "2           62.0  35.65807  139.751602        62.0              8.0   \n",
       "3           62.0  35.65807  139.751602        62.0              8.0   \n",
       "4           62.0  35.65807  139.751602        62.0              8.0   \n",
       "\n",
       "   air_area_name0  air_genre_name1  air_area_name1  air_genre_name2  \\\n",
       "0            62.0              0.0             0.0              0.0   \n",
       "1            62.0              0.0             0.0              0.0   \n",
       "2            62.0              0.0             0.0              0.0   \n",
       "3            62.0              0.0             0.0              0.0   \n",
       "4            62.0              0.0             0.0              0.0   \n",
       "\n",
       "   air_area_name2  air_genre_name3  air_area_name3  air_genre_name4  \\\n",
       "0             0.0              0.0             0.0              0.0   \n",
       "1             0.0              0.0             0.0              0.0   \n",
       "2             0.0              0.0             0.0              0.0   \n",
       "3             0.0              0.0             0.0              0.0   \n",
       "4             0.0              0.0             0.0              0.0   \n",
       "\n",
       "   air_area_name4  air_genre_name5  air_area_name5  air_genre_name6  \\\n",
       "0             0.0              0.0             0.0              0.0   \n",
       "1             0.0              0.0             0.0              0.0   \n",
       "2             0.0              0.0             0.0              0.0   \n",
       "3             0.0              0.0             0.0              0.0   \n",
       "4             0.0              0.0             0.0              0.0   \n",
       "\n",
       "   air_area_name6  air_genre_name7  air_area_name7  air_genre_name8  \\\n",
       "0             0.0              0.0             0.0              0.0   \n",
       "1             0.0              0.0             0.0              0.0   \n",
       "2             0.0              0.0             0.0              0.0   \n",
       "3             0.0              0.0             0.0              0.0   \n",
       "4             0.0              0.0             0.0              0.0   \n",
       "\n",
       "   air_area_name8  air_genre_name9  air_area_name9  rs1_x  rv1_x  rs2_x  \\\n",
       "0             0.0              0.0             0.0   -1.0   -1.0   -1.0   \n",
       "1             0.0              0.0             0.0   -1.0   -1.0   -1.0   \n",
       "2             0.0              0.0             0.0   -1.0   -1.0   -1.0   \n",
       "3             0.0              0.0             0.0   -1.0   -1.0   -1.0   \n",
       "4             0.0              0.0             0.0   -1.0   -1.0   -1.0   \n",
       "\n",
       "   rv2_x  rs1_y  rv1_y  rs2_y  rv2_y  total_reserv_sum  total_reserv_mean  \\\n",
       "0   -1.0   -1.0   -1.0   -1.0   -1.0              -1.0               -1.0   \n",
       "1   -1.0   -1.0   -1.0   -1.0   -1.0              -1.0               -1.0   \n",
       "2   -1.0   -1.0   -1.0   -1.0   -1.0              -1.0               -1.0   \n",
       "3   -1.0   -1.0   -1.0   -1.0   -1.0              -1.0               -1.0   \n",
       "4   -1.0   -1.0   -1.0   -1.0   -1.0              -1.0               -1.0   \n",
       "\n",
       "   total_reserv_dt_diff_mean  date_int  var_max_lat  var_max_long  \\\n",
       "0                       -1.0  20160113     8.362564      4.521799   \n",
       "1                       -1.0  20160114     8.362564      4.521799   \n",
       "2                       -1.0  20160115     8.362564      4.521799   \n",
       "3                       -1.0  20160116     8.362564      4.521799   \n",
       "4                       -1.0  20160118     8.362564      4.521799   \n",
       "\n",
       "   lon_plus_lat  air_store_id2  \n",
       "0    175.409668            603  \n",
       "1    175.409668            603  \n",
       "2    175.409668            603  \n",
       "3    175.409668            603  \n",
       "4    175.409668            603  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "train[col].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! our data preparation is done. Let's move on to model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d89591c9-bced-4a06-8503-fc795f3d20fb",
    "_uuid": "d19d1ddea592664439ed0cc603581604cea78e09"
   },
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "1c08bde2-f16d-450f-9927-b45231193fee",
    "_uuid": "ab7f3e82025fc56afb7d2e6437617cb454d45c5a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#error metric\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred)**0.5\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train=train[col]\n",
    "y_train=np.log1p(train['visitors'])\n",
    "\n",
    "#X_train_grid,X_valid,y_train_grid,y_valid=train_test_split(X_train,y_train,random_state=1,train_size=0.75,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "94a73c2f-3eeb-4837-9ba3-ba74e5875d8e",
    "_uuid": "cdac4ef37528b7b9c8b78c7c1595952b366a0df9"
   },
   "source": [
    "Per scikit-learn [documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html):\n",
    "> The `randomized search` and the `grid search` explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_cell_guid": "b9993387-f9e3-42c3-92a3-4b1f05ef54c3",
    "_uuid": "a31cef16727dfc0eb2e33df4d9d76296159851d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.489665458467073e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "best_params={}\n",
    "best_score={}\n",
    "n_iter_search = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rand_grid_xgb={'learning_rate':[0.05,0.1,0.2],\n",
    "               'n_estimators':[200,400],\n",
    "               'max_depth':[5,10,20],\n",
    "               'subsample':[0.7,0.8],\n",
    "              'colsample_bytree':[0.7,0.8]}\n",
    "\n",
    "model_xgb=XGBRegressor()\n",
    "grid_xgb=RandomizedSearchCV(model_xgb,param_distributions=rand_grid_xgb,scoring='neg_mean_squared_error',n_iter=n_iter_search)\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "best_params(xgb)=grid_xgb.best_params_\n",
    "best_score(xgb)=grid_xgb.best_score_\n",
    "\n",
    "preds_xgb=grid_xgb.predict(X_train)\n",
    "\n",
    "print('Cross-validated RMSE XGBRegressor: ', RMSE(y_train, preds_xgb))\n",
    "\n",
    "print (time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61d043a8-da25-4fd6-9da7-a78fb6090292",
    "_uuid": "ac993ed35a53ee6eb5c42bd5fabac5364e3cc480",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_gbr=ensemble.GradientBoostingRegressor()\n",
    "rand_grid_gbr={'learning_rate':[0.05,0.1,0.2],\n",
    "               'loss':['ls','lad','huber'],\n",
    "                'n_estimators':[200,400],\n",
    "               'max_depth':[5,10,20],\n",
    "               'subsample':[0.7,0.8],\n",
    "              'alpha':[0.9,0.7]}\n",
    "grid_gbr=RandomizedSearchCV(model_gbr,param_distributions=rand_grid_gbr,scoring='neg_mean_squared_error',n_iter=n_iter_search)\n",
    "grid_gbr.fit(X_train, y_train)\n",
    "\n",
    "best_params(gbr)=grid_gbr.best_params_\n",
    "best_score(gbr)=grid_gbr.best_score_\n",
    "\n",
    "preds_gbr=grid_gbr.predict(X_valid)\n",
    "\n",
    "print('RMSE GradientBoostingRegressor: ', RMSLE(y_valid, preds_gbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea0c28fe-a694-4416-8f7a-77a219030afc",
    "_uuid": "a4d248d51ccf5a4b8f583682a0115afe2aa9abc5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_kn=neighbors.KNeighborsRegressor()\n",
    "rand_grid_kn={'n_jobs':-1,\n",
    "               'n_neighbors':[3,4,5,8],\n",
    "               'weights':['uniform','distance']}\n",
    "\n",
    "grid_kn=RandomizedSearchCV(model_kn,param_distributions=rand_grid_kn,scoring='neg_mean_squared_error',n_iter=n_iter_search)\n",
    "grid_kn.fit(X_train, y_train)\n",
    "\n",
    "best_params(kn)=grid_gbr.best_params_\n",
    "best_score(kn)=grid_gbr.best_score_\n",
    "\n",
    "preds_kn=grid_kn.predict(X_valid)\n",
    "\n",
    "print('RMSE KNeighborsRegressor: ', RMSLE(y_valid, preds_kn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "59873110-c4e2-42cc-ae2b-619d0a8a17bc",
    "_uuid": "c8b809bf42353fb679741afd552944a682ba569f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_et=ExtraTreesRegressor()\n",
    "rand_grid_et={\n",
    "    'n_jobs':-1,\n",
    "    'n_estimators':[200,1000,10000],\n",
    "    'max_depth':[5,10,20],\n",
    "    'min_samples_leaf':[100,150,300],\n",
    "    'max_features':[0.7,0.8]}\n",
    "\n",
    "grid_et=RandomizedSearchCV(model_et,param_distributions=rand_grid_et,scoring='neg_mean_squared_error',n_iter=n_iter_search)\n",
    "grid_et.fit(X_train, y_train)\n",
    "\n",
    "best_params(et)=grid_et.best_params_\n",
    "best_score(et)=grid_et.best_score_\n",
    "\n",
    "preds_et=grid_et.predict(X_valid)\n",
    "\n",
    "print('RMSE ExtraTreesRegressor: ', RMSLE(y_valid, preds_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8fd45759-4f19-404a-8e7f-6c76731adaa4",
    "_uuid": "e50ee7e37098ecaec2bb92454650484bf831153f",
    "collapsed": true
   },
   "source": [
    "#### Predicting on all rows of the test set\n",
    "Now, we predict each best model on all of the rows of tes set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "936ae028-cdc1-470b-bbd0-5a304a504366",
    "_uuid": "f77595b354205831585adc08a062a11a982c37ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "print(best_score)\n",
    "\n",
    "best_params.to_csv('best_params.csv')\n",
    "best_score.to_csv('best_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b65ce4c2-e5c8-4f9c-8fc3-b03973d78805",
    "_uuid": "8dbd089a2ce19b7631add39430a8b6f43df667de",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_xgb_test=grid_xgb.predict(test[col])\n",
    "preds_gbr_test=grid_gbr.predict(test[col])\n",
    "preds_kn_test=grid_kn.predict(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "31ce4f1a-52b9-4ec4-ab06-2d51e5fe2ff2",
    "_uuid": "39e0ec39c95800e49a0cb7177d82b64d5e0e0e42",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['visitors'] = 0.3*preds_kn_test+0.3*preds_gbr_test+0.4*preds_xgb_test\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "#del train; del data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "859d88bd-c557-4959-8edf-bb0b188bfaea",
    "_uuid": "f161e04941f4adc83ff057bd17616ee23a13bcc4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../input/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "\n",
    "sub_merge['visitors'] = 0.7*sub_merge['visitors_x'] + 0.3*sub_merge['visitors_y']* 1.2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
